% -*- mode: noweb; noweb-default-code-mode: R-mode; -*-
%\VignetteIndexEntry{An R package for fitting probabilistic index models}
%\VignetteKeyword{probabilistic index model, regression}
%\VignetteDepends{pim}
%\VignettePackage{pim}
%documentclass[12pt, a4paper]{article}
\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage[authoryear,round]{natbib}
\usepackage[utf8]{inputenc}

 
\textwidth=6.2in
\textheight=8.5in
%\parskip=.3cm
\oddsidemargin=.1in
\evensidemargin=.1in
\headheight=-.3in

\newcommand{\prob}[1]{\text{P}\left\{#1\right\}}
\newcommand{\hatprob}[1]{\hat{\text{P}}\left\{#1\right\}}
\newcommand{\I}[1]{\text{I}\left\{#1\right\}}
\newcommand{\leqs}{\preccurlyeq}
\newcommand{\pim}[1]{\texttt{#1}}%terms refering to the pim package itself
\newcommand{\cd}[1]{\texttt{#1}}%code or similar (that is not in a math environment)



\author{Jan De Neve}
\usepackage{Sweave}
\begin{document}
\input{pim-concordance}
\title{pim: An R package for fitting probabilistic index models}

\maketitle
\tableofcontents
\section{Introduction}\label{S_intro}

This document explains and illustrates how the \pim{pim}-package can be employed to fit a Probabilistic Index Model (PIM). We refer to \cite{Thas2012} for a detailed overview on PIMs. If $(Y,X)$ and $(Y',X')$ are i.i.d. then a PIM is defined as
\begin{equation}\label{pim}
\prob{Y \leqs Y' | X, X'} = m(X,X';\beta) = g^{-1}(Z^T \beta) \quad \text{for} \quad (X,X') \in \mathcal{X},
\end{equation}
with $\prob{Y \leqs Y'} \equiv  \prob{Y < Y'} + \frac{1}{2} \prob{Y = Y'}$. Here $g(\cdot)$ denotes a link function, $Z$ is a covariate vector that depends on the predictors $X$ and $X'$ and $\mathcal{X}$ is the set of predictors for which the model is defined. 

The \pim{pim}-package allows fitting a nearly unlimited range of PIMs through extensive customisations.
\begin{enumerate}
\item One can manually provide the set of pairs of observation indices for the pseudo-observations (the "poset"), one can use any of the provided functions (\pim{onewayposet} which includes all unique oneway combinations ($(1,2)$, $(1,3)$ and $(2,3)$), \pim{pairwiseposet} which does the same after ordering the data based on the predictors in the model (the lexicographical order restriction) or the default \pim{fullposet} which simply contains all combinations), and one can even write a custom function for it.

For example, in the presence of 2 predictors, say $X^T = (X_1, X_2)$, the lexicographical order restricted model is defined for $\mathcal{X} = \left\{ (X,X') | X_1 < X'_1 \text{ or if } X_1 = X'_1 \text{ then } X_2 \leq X'_2 \right\}$, which can be selected straightforwardly by employing the pairwiseposet function. 
\item The link function in the current implementation is restricted to \cd{"identity"}, \cd{"logit"} and \cd{"probit"}. However, through customisation of the estimators (in particular by providing custom implementations of \pim{scorefunctioncreator.default} and \pim{Uforposandwich.default}), this can be easily overcome.
\item By default, the left hand side of the formula (e.g. $y$ in $y\sim x$) is always used for a true probabilistic index $\prob{y \leqs y' }$, but $\prob{y \leq y' }$ and $\prob{y < y' }$ can also be attained through parameter \cd{lhs}.
\item In the presence of categorical predictors transitivity is assumed.
\end{enumerate} 

The function \pim{pim} allows fitting PIMs for different choices of $Z$. A natural choice is the difference in predictors, i.e. $Z = X' - X$. Unless its parameter \cd{interpretation} is set to \cd{"marginal"}, all predictors (e.g. $X_1$) that occur in the model formula without altering functions (see below) are indeed interpreted as $X_1' - X_1$. For interactions to also behave as the difference, an extra parameter \cd{interactions.difference} is provided. The defaults are chosen in such a way that the design matrix is created as the difference between the left and right design matrix, but with an intercept added. If you want to avoid the intercept, you have to exclude it from the model as you would in normal formulas, by adding $-1$ to it.

As an example, model formula $y\sim a*b$ will (by default) represent $\prob{y \leqs y' }=\beta_0+\beta_1  (a'-a) + \beta_2  (b'-b) + \beta_3  (a'b' - ab)$.

Note that the above interpretation (i.e. with parameter \cd{interpretation} equal to \cd{"regular"}) of the model formula will only interpret individual columns and interactions as differences. If you want to completely enforce the design matrix to be the difference of the design matrices, you can use the default \cd{interpretation="difference"}, which will indeed enforce this. The altering functions are not allowed in this case, and \cd{interactions.difference} is ignored.

Note that when the model satisfies $m(X,X';\beta) + m(X',X;\beta) = 1$ the lexicographical order restriction corresponds to the NO order restriction and hence the model is defined for all couples of predictors $(X,X')$, see \cite{Thas2012} for more details. 

For expressing more complex models, 4 altering functions are provided: $L(X)$, $R(X)$, $O(X)$ and $F(X)$. These expand to:
\begin{enumerate}
\item $L(X)$: the $X$ value of only the left part of the pseudo-observation (with the default suffixes provided, this will be denoted further as $X \_ L$)
\item $R(X)$: the $X$ value of only the right part of the pseudo-observation (with the default suffixes provided, this will be denoted further as $X \_ R$)
\item $O(X)$: (can only be used on orderable predictors) $I\left( L(X) \leqs R(X) \right)$
\item $F(X)$: (can only be used on factors) holds all interaction terms where the left value is smaller than the right one.
\end{enumerate} 

Finally, when \emph{force.marginal} is \emph{TRUE}, terms $X$ without altering functions are interpreted as $R(X)$. This is typically only useful for marginal models, as specified in TODO rankpaper. Note that some of the altering functions are not relevant in marginal models, and the fit will fail if you try to do so.

In the following sections we illustrate the \texttt{pim()} function according to different choices of $Z$. In Sections \ref{S_crds}-\ref{S_fe} case studies from Section 6 in \cite{Thas2012} are analyzed, while Section \ref{S_categorical} considers categorical predictors. Section \ref{S_conclusion} gives some conclusions and remarks.  


\section{pim parameters and their influence}\label{S_parms}
As can be observed in the help for the \pim{pim} function (\cd{?pim}), a lot of parameters have been provided. Some of these require a function passed in themselves, so a large amount of customisation is possible. This section describes in some more detail the effect of each of the parameters.
\subsection{Standard parameters}\label{SS_spar}
The \pim{pim} function currently supports only the case where a \cd{data.frame}-like object can be passed through \cd{data}, and the model can be expressed as a \cd{formula}. If you want to manually provide the design matrix, you need to create an object of class \cd{pimfitdata} yourself (see \cd{?pim.fit.prep}) and pass this to \pim{pim.fit}.

The other more traditional parameters for a model fitting function are \cd{link} (providing one of the link functions - note that although the provided options suggest otherwise, currently only \cd{"logit"}, \cd{"identity"} and \cd{"probit"} are provided, where \cd{"logit"} is the default), and \cd{na.action} (note again that no effort has been made to handle missing data in the fitting code, so there will be little to no use in letting \cd{NA} values pass. Also: the \cd{na.action} is applied to the original \cd{data}, not to the design matrix, so it is typically the strictest possible.)

We have opted to allow for blocking variables not through constructs in the formula, but by simply providing a \cd{character} vector \cd{blocking.variables}. Note that these are only used to filter the \cd{poset}: only combinations of observations that share the same values for the blocking variables are allowed.

Finally, we provide a parameter \cd{verbosity}: most of the functions within this package supprt it, and it is typically passed on down the stack of functions while diminishing it. At some points, when this variable is above a threshold (typically, above zero), some diagnostic or progress text is displayed. For long running fits, this can be interesting to follow the progress (and then the overhead of the continuous logging will also be relatively small). It can also be used to investigate unexpected results, as it may display intermediate results, so the cause for the unexpected result can typically be pinpointed more quickly. Please be aware that the verbosity comes at a performance cost, so for typical use it is best to leave \cd{verbosity} at its default of zero.

\subsection{Design parameters}\label{SS_design}
There are quite a few parameters that govern how the formula is interpreted to create the design matrix.
\subsubsection{\cd{poset}}
This parameter represents either a matrix (or similar structure, though experience shows that matrix is indeed the fastest performing) with two columns holding the rownumbers of the left and right observation in each pseudo-observation (in \cite{Thas2012}, this is denoted with a calligraphic $I$.), or a function that can create this based on the data.

We expect that the option to immediately pass in a matrix will be seldom used.

If you pass a function, it should have three parameters: \cd{data}, \cd{formula} and \cd{verbosity}. The values it will receive are always the ones that were passed along to \pim{pim}. The function should return a list with two items: \cd{data} (which should hold either the original data or some reordered or transformed version of it) and \cd{poset}, which should be the resulting matrix of rownumber combinations. Some functions have already been provided for use in this way, which will be able to handle most cases:
\begin{itemize}
	\item \pim{fullposet}: the default option, which will simply create a matrix with all combinations of rownumbers.
	\item \pim{onewayposet}: this will contain only combinations where the left rownumber is smaller than the right one
	\item \pim{pairwiseposet}: will first try to order the data according to the variables in the model (note: if more than one variables is present, the order of the variables is the one in the dataset), and then again will return only combinations where the left rownumber is smaller than the right one. In \cite{Thas2012}, this is known as the lexicographical order.
	\item \pim{forcedcolorderonewayposet}\cd{(columnnames)}: here, the data is first ordered according to the \cd{columnnames} passed in, and then again returns only combinations where the left rownumber is smaller than the right one.
	\item \pim{oldpimposet} and \pim{oldpimposetbft} are provided for backward compatibility and should not be used.
\end{itemize}
\subsubsection{\cd{leftsuffix},\cd{rightsuffix}}
During the creation of the design matrix, it often happens that two versions of the same variable have to be handled, pertaining to the left and right observation. Where necessary, these will be referred to with their original variable names with the matching suffixes appended. The defaults (\cd{\_L} and \cd{\_R}) will probably suffice for most circumstances (and little to no testing has been done with other suffixes). These parameters are mostly provided for the unlikely event where both columns \cd{X} and \cd{X\_L} are already present in the data. Some very basic safety checks are performed in \pim{pimformula} to avoid issues here.
\subsubsection{\cd{lhs}}
A regular PIM model will always have $\prob{Y \leqs Y' }$ as its pseudo-outcome, which will be the interpretation of the left hand side of the \cd{formula} if \cd{lhs="PO"}, the default. For other applications, we also provide $\prob{Y \leq Y' }$ and $\prob{Y < Y' }$, by passing \cd{"<="} or \cd{"<"} as \cd{lhs}. Note that in the future, this may be extended (see \pim{pimformula}) to include even more general functions.
\subsubsection{\cd{interpretation} and \cd{interactions.difference}}
These are probably the most influential parameters. \cd{interactions.difference} is ignored unless \cd{interpretation} is \cd{"regular"}, so the existing combinations are:
\begin{itemize}
	\item {\cd{interpretation="difference"}}: In this case, the formula is used to create a \cd{glm} style design matrix of all the original observations (see \cd{model.matrix}). Then the results for the left and right observations are subtracted (though if an intercept was present, this is left out!). It is important to realise that specifying an intercept or not in the \cd{formula} will influence the way the original design matrix is created for factor variables, so consider this carefully. We expect that you will want to include the intercept in most cases, so that the first level of the factor is used as a reference (and gets no dummy variable).
	\item {\cd{interpretation="regular"} and \cd{interactions.difference=TRUE}} In this case, main effect terms that are variable names occurring in the \cd{data}, and interaction terms are interpreted as differences between the right and left values of that term, where possible. If no calculated columns are present, this should have the same effect as \cd{interpretation="difference"}, except that intercepts are not excluded. Calculated main effect terms (e.g. \cd{I(X\^2)}) are interpreted by replacing each column name with the difference (in the example: \cd{$(X'-X)^2$}), where variables are first converted to numerical (note the impact for factors!!). In addition, the altering functions \cd{L,R,F,O} can be used as shortcuts to some calculated terms (see the end of Section \ref{S_intro})
	\item {\cd{interpretation="regular"} and \cd{interactions.difference=FALSE}} This is the same as the previous option, but interaction terms are now treated the same way as calculated terms. The simplest example where the difference is clear, is in the interpretation of $Y\sim A*B$: with \cd{interactions.difference=TRUE}, the interaction term is interpreted as $I((A\_R:B\_R)-(A\_L:B\_L))$, while with \cd{interactions.difference=FALSE}, it is interpreted as $(A\_R-A\_L):(B\_R-B\_L)$.
	\item {\cd{interpretation="marginal"}}: in this interpretation, only the right side value of each variable is allowed in the model. As such, variable names are replaced with their right side value. Some of the altering functions are still allowed, but have slightly different forms (see again at the end of Section \ref{S_intro}).
\end{itemize}
\subsection{Fitting parameters}\label{SS_fit}
Once the model matrix has been created and the link function is known, fitting the PIM requires estimating the coefficients through solving a set of (potentially nonlinear) equations and then estimating the variance.

For estimating the coefficients, several methods are readily provided, that can be passed as the \cd{estimator} parameter to \pim{pim}:
\begin{itemize}
	\item \pim{estimator.nleqslv} (the default): will use \cd{nleqslv} to solve the equations, and as such, all parameters that tune its performance can be passed along. In addition, \cd{ignore.error} can be set to \cd{TRUE}, so nonconvergence will be ignored (although we encourage users to solve this through the other parameters to \cd{nleqslv}). The function that calculates the lefthand side of the equations (i.e. what has to be set to zero) based on the \cd{link} function has to be provided as the \cd{scoreFunctionCreator}. Its default, \pim{scorefunctioncreator.default} provides these for \cd{"logit"}, \cd{"identity"} and \cd{"probit"}, but this can be easily extended to other link functions by employing equation 8 of \cite{Thas2012} (see the implementation of \cd{scorefunctioncreator.default} for the requirements of this type of function). Note also that the set of equations for a given link function is uniquely defined, and the current implementation does not claim to hold the most efficient set.
	\item \pim{estimator.BB}: very similar to \cd{estimator.nleqslv}, although this relies on \cd{BBsolve} from package \cd{BB}, so the parameters reflect this.
	\item \pim{estimator.glm}: the maximum likelihood estimating equations for the mathing \cd{glm} are of the form of equation 8 in \cite{Thas2012}, so any solution to these will be a correct estimate of the PIM coefficients. This is what this estimator provides. A slight disadvantage of this is that the \cd{glm} (co)variance estimate is also calculated although this is not actually usable in most cases (since it requires independence of the pseudo-observations). This can be ignored, but might involve a performance impact for sizeable data.
	\item \pim{estimator.trymultiple}: this will try all reasonable parameter values for \cd{nleqslv}, and after that \cd{BBSolve} until one works (i.e. does not give an error). This can obviously be very slow, so it is much better to figure out which set of parameters works for a given dataset. This is only intended for lazy people who have plenty of time on their hands.
	\item \pim{estimator.glmnet}: this estimator is still somewhat in the experimental phase, but will apply elastic net penalization to the estimating equations by employing \cd{glmnet} in a similar manner as \pim{estimator.glm} does with \cd{glm}. The parameters are the natural ones to \cd{glmnet}.
\end{itemize}

When estimating the sample (co)variances of these parameter estimates, we also have several options. It should be noted that a requirement for the (co)variance estimates to be correct is that the same set of estimating equations is used. This is in no way enforced by \pim{pim}! Because the coefficient estimation occurs completely seperate, and custom functions that represent the equations can be provided, there is no way to keep these in check. This is the responsability of the user of these functions. Note, however that for the provided link functions (\cd{"logit"}, \cd{"identity"} and \cd{"probit"}) and the default estimating functions, this is OK.

The (co)variance estimating options are:
\begin{itemize}
	\item \pim{varianceestimator.sandwich} (the default): provides a straightforward and highly optimized implementation of the sandwich estimator (theorem 2 in \cite{Thas2012}). Similarly as for \pim{estimator.nleqslv}, a function \cd{Uforposandwich} providing the estimating equations and their partial derivatives has to be procured. Once again, the default for this parameter (\pim{Uforposandwich.default}) provides the matching results for the three provided link functions and \pim{scorefunctioncreator.default}.
	\item \cd{NULL}: by passing \cd{NULL}, no attempt is made to estimate the (co)variances. This can be useful in e.g. bootstrapping or crossvalidating settings, where these calculations would cause unnecessary overhead.
	\item \pim{varianceestimator.H0}: when the link function is \cd{"identity"}, a simpler and more efficient estimate of the (co)variances exists when the null hypothesis that all parameters are zero is true. This estimator provides just that estimate.
	\item \pim{varianceestimator.glm}: can only be used if the coefficient estimation happened through \pim{estimator.glm}: in that case, this estimator returns \cd{glm}'s (co)variance estimate. The user should take care only to use this when the design implies independence of the psuedo-observations.
	\item \pim{varianceestimator.bootstrap}: the variance can also be estimated by a nonparametric bootstrap (important notice: the bootstrap samples are taken on the original dataset (not simply on the pseudo-observations) to ensure that the covariance pattern is preserved). Besides taking the number of bootstrap iterations (\cd{D}) as a parameter, you can also specify \cd{keep.posetbs=TRUE} (the default being \cd{FALSE}) to also return a list of the resampled pseudo-observations (note: the implementation assumes that all pseudo-observations that can be attained through the poset mechanism on the bootstrap samples, were already present in the orginal poset). You can look at the code of the nonexported function \cd{.basicbootstrap} (\cd{getAnywhere(".basicbootstrap")}) to see how to use this to obtain a bootstrapped \pim{pimfitdata} object from the original one.
\end{itemize}
\subsection{Make-up parameters}\label{SS_beauty}
Especially with calculated variables, but even when using just the default settings, the names of variables can become quite cluttered, since they now typically involve the differences between the right and left values. Some attempts are taken to make the variable names more readable:

\cd{nicenames}: if this is \cd{TRUE} (the default), the mechanism to make the names more readable is activated. This includes automatic renaming of the differences (with the default suffixes, \cd{X\_R-X\_L} is then renamed to \cd{X\_R-\_L}) as well as proper names for the results of the altering functions.
\cd{extra.nicenames}: through this dataset, extra variable names (or parts of them) and matching "nicer" names can be passed in to perform the renaming. It is useful to know that the whitespace will be removed from names before attempting a replace. An example on how to use this can be found in \cd{?pim}: see \cd{pimb} there, or also Section \label{S_fe} below.
\subsection{Utility parameters}\label{SS_utility}
One final parameter is \cd{keep.data}, which chooses whether or not the design matrix is kept in the final object. We expect that especially for big models, this might be rather big, so the default is not to keep it.
\section{Childhood respiratory disease study}\label{S_crds}

For the childhood respiratory disease study we consider the PIM with interaction
\begin{eqnarray*}
\text{logit} \left( \prob{FEV \leqs FEV' } \right) &=& \beta_1 (AGE' - AGE) + \beta_2(SMOKE' - SMOKE) \\ 
												   & &  + \beta_3 (AGE'*SMOKE' - AGE*SMOKE). 
\end{eqnarray*}
Because this PIM corresponds to a covariate vector $Z$ of the form $Z = X' -X$, with $X^T = (AGE, SMOKE, AGE*SMOKE)$, the formula statement of \texttt{pim()} is similar to the formula statement of \texttt{lm()} and \texttt{glm()}. We first read in the data
\begin{Schunk}
\begin{Sinput}
> library(pim)
> data("FEVData")
> head(FEVData)
\end{Sinput}
\begin{Soutput}
  Age   FEV Height Sex Smoke
1   9 1.708   57.0   0     0
2   8 1.724   67.5   0     0
3   7 1.720   54.5   0     0
4   9 1.558   53.0   1     0
5   9 1.895   57.0   1     0
6   8 2.336   61.0   0     0
\end{Soutput}
\end{Schunk}
Here \verb|FEV| stands for the forced expiratory volume ($FEV$), \verb|Age| for the age of the child ($AGE$) and \verb|Smoke| whether a child smokes or not ($SMOKE$). We fit the PIM:
\begin{Schunk}
\begin{Sinput}
> library('pim')
> pim.fit1 <- pim(FEV ~ Age*Smoke-1, data = FEVData, link="logit", 
+   poset=oldpimposet, estimator=estimator.nleqslv(ignore.error=TRUE), 
+   keep.data=TRUE, interpretation="regular")
> pim.fit1
\end{Sinput}
\begin{Soutput}
Call:
pim(formula = FEV ~ Age * Smoke - 1, data = FEVData, link = "logit", 
    poset = oldpimposet, interpretation = "regular", estimator = estimator.nleqslv(ignore.error = TRUE), 
    keep.data = TRUE)

Coefficients:
      Age_R-_L     Smoke_R-_L Age:Smoke_L-_R 
     0.6076003      5.3068852     -0.4553885 
\end{Soutput}
\end{Schunk}
The estimated model is given by
\begin{eqnarray*}
\text{logit} \left( \hatprob{FEV \leqs FEV' } \right) &=& 0.61 (AGE' - AGE) + 5.31(SMOKE' - SMOKE) \\ 
												   & &   -0.46 (AGE'*SMOKE' - AGE*SMOKE). 
\end{eqnarray*}

Thus the \texttt{lm()}-like formula 
\begin{verbatim}
 ~ Age*Smoke = Age + Smoke + Age:Smoke,
\end{verbatim}
is automatically converted to a \texttt{pim()}-like formula
\begin{verbatim}
~ (Age' - Age) + (Smoke' - Smoke) + (Age':Smoke' - Age:Smoke).
\end{verbatim}
The \texttt{summary()} function gives the estimates and corresponding standard errors together with the $Z-$ and p-value corresponding to the null-hypothesis $H_0: \beta = 0$. 
\begin{Schunk}
\begin{Sinput}
> summary(pim.fit1)
\end{Sinput}
\begin{Soutput}
Call:
pim(formula = FEV ~ Age * Smoke - 1, data = FEVData, link = "logit", 
    poset = oldpimposet, interpretation = "regular", estimator = estimator.nleqslv(ignore.error = TRUE), 
    keep.data = TRUE)

                Estimate Std. Error Z value  Pr(>|z|)    
Age_R-_L        0.607600   0.030124 20.1697 < 2.2e-16 ***
Smoke_R-_L      5.306885   1.044227  5.0821 3.732e-07 ***
Age:Smoke_L-_R -0.455388   0.078543 -5.7979 6.714e-09 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
\end{Soutput}
\end{Schunk}
The \texttt{plot()} function provides a rudimentary goodness-of-fit plot.
\begin{center}
\begin{Schunk}
\begin{Sinput}
> plot(pim.fit1)
\end{Sinput}
\end{Schunk}
\includegraphics{pim-004}
\end{center}
The functions \texttt{coef()}, \texttt{vcov()} and \texttt{fitted.values()} provide the estimated coefficients, variance-covariance matrix of $\hat{\beta}$ and the fitted values respectively. 
\begin{Schunk}
\begin{Sinput}
> coef(pim.fit1)
\end{Sinput}
\begin{Soutput}
      Age_R-_L     Smoke_R-_L Age:Smoke_L-_R 
     0.6076003      5.3068852     -0.4553885 
\end{Soutput}
\begin{Sinput}
> vcov(pim.fit1)
\end{Sinput}
\begin{Soutput}
                    Age_R-_L   Smoke_R-_L Age:Smoke_L-_R
Age_R-_L        0.0009074760  0.009485251  -0.0009254122
Smoke_R-_L      0.0094852506  1.090409267  -0.0802054923
Age:Smoke_L-_R -0.0009254122 -0.080205492   0.0061690504
\end{Soutput}
\begin{Sinput}
> head(fitted.values(pim.fit1))
\end{Sinput}
\begin{Soutput}
            [,1]
26_222 0.5000000
26_23  0.6473932
222_23 0.6473932
26_59  0.6473932
222_59 0.6473932
23_59  0.5000000
\end{Soutput}
\end{Schunk}

We end this section with an illustration of the interpretation of the age effect. For 2 randomly selected children with the same smoking status and a year difference in age, the probability that the eldest has a higher FEV is estimated by $\text{expit}(0.61 - 0.46SMOKE)$. For non-smokers this probability is $0.65$, while for smokers this becomes $0.54$.

\section{Mental health study}

For the mental health study the following PIM was proposed
\begin{equation}\label{pim.mhs}
\text{logit}\left(\prob{MI \leqs MI'} \right) = \beta_1 (SES' - SES) + \beta_2 (LI' - LI). 
\end{equation}
\begin{Schunk}
\begin{Sinput}
> data("MHData")
> head(MHData)
\end{Sinput}
\begin{Soutput}
  mental ses life
1      1   1    1
2      1   1    9
3      1   1    4
4      1   1    3
5      1   0    2
6      1   1    0
\end{Soutput}
\end{Schunk}
Here \verb|mental| stands for the mental impairment ($MI$), \verb|ses| for the socioeconomic status ($SES$) and \verb|life| for the life index ($LI$). Similar as in the previous example we can specify a \texttt{lm()}-like formula. 
\begin{Schunk}
\begin{Sinput}
> pim.fit2a <- pim(mental ~ ses + life -1, data = MHData, link="logit", 
+   poset=oldpimposet, estimator=estimator.nleqslv(ignore.error=TRUE), 
+   keep.data=TRUE, interpretation="regular")
> summary(pim.fit2a)
\end{Sinput}
\begin{Soutput}
Call:
pim(formula = mental ~ ses + life - 1, data = MHData, link = "logit", 
    poset = oldpimposet, interpretation = "regular", estimator = estimator.nleqslv(ignore.error = TRUE), 
    keep.data = TRUE)

           Estimate Std. Error Z value Pr(>|z|)   
ses_R-_L  -0.740163   0.343575 -2.1543 0.031217 * 
life_R-_L  0.201179   0.073371  2.7419 0.006108 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
\end{Soutput}
\end{Schunk}
The model is estimated by
\[
\text{logit}\left(\hatprob{MI \leqs MI'} \right) = -0.74 (SES' - SES) + 0.2 (LI' - LI). 
\]

Model (\ref{pim.mhs}) can be extended as follows
\begin{eqnarray*}
\text{logit}\left(\prob{MI \leqs MI'} \right) = \beta_1 (SES' - SES) + \beta_2 (LI' - LI) + \beta_3 SES + \beta_4 LI. 
\end{eqnarray*}
If we want to fit this model, we need to specify the \texttt{formula} statement explicitly because $Z$ is no longer of the form $Z = X' - X$. Some notation is needed to specify the predictors corresponding to the left response in $\prob{MI \leqs MI'}$, thus $(SES,LI)$ and the predictors corresponding to the right response, thus $(SES', LI')$. The altering functions can be used for this: \texttt{L()} for the predictors corresponding to the left response and \texttt{R()} for the predictors corresponding to the right response. Thus $(SES, LI)$ in R becomes \texttt{(L(ses), L(life)} and $(SES',LI')$ becomes \texttt{(R(ses), R(life)}. The \texttt{I()} statement is needed to specify specific functions. The function
\[
\beta_1 (SES' - SES) + \beta_2 (LI' - LI) + \beta_3 SES + \beta_4 LI,
\]
in R becomes
\begin{center}
\begin{verbatim}
 ~ ses + life + L(ses) + L(life) - 1
\end{verbatim}
\end{center}
\begin{Schunk}
\begin{Sinput}
> pim.fit2b <- pim(mental ~ ses + life + L(ses) + L(life) - 1, data = MHData,  
+   link="logit", poset=oldpimposetbft,  
+   estimator=estimator.nleqslv(ignore.error=TRUE),keep.data=TRUE, 
+   interpretation="regular")
> summary(pim.fit2b)
\end{Sinput}
\begin{Soutput}
Call:
pim(formula = mental ~ ses + life + L(ses) + L(life) - 1, data = MHData, 
    link = "logit", poset = oldpimposetbft, interpretation = "regular", 
    estimator = estimator.nleqslv(ignore.error = TRUE), keep.data = TRUE)

           Estimate Std. Error Z value Pr(>|z|)   
ses_R-_L  -0.670723   0.382665 -1.7528 0.079642 . 
life_R-_L  0.205459   0.069989  2.9356 0.003329 **
ses_L     -0.034676   0.163157 -0.2125 0.831693   
life_L    -0.021601   0.039843 -0.5422 0.587711   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
\end{Soutput}
\end{Schunk}
The estimated model is given by
\begin{eqnarray*}
\text{logit}\left(\hatprob{MI \leqs MI'} \right) = -0.67 (SES' - SES) + 0.21 (LI' - LI)  -0.03 SES  -0.02 LI. 
\end{eqnarray*}




\section{Food expenditure study}\label{S_fe}

Because of heteroscedasticity the following PIM is proposed to analyze the food expenditure data.
\[
\text{logit}\left( \prob{FE \leqs FE' } \right) = \beta \frac{HI' - HI}{\sqrt{HI' + HI}}.  
\]
\begin{Schunk}
\begin{Sinput}
> data("Engeldata")
\end{Sinput}
\end{Schunk}
Here \verb|income| denotes the household income ($HI$) while \verb|foodexp| denotes the food expenditure ($FE$). The covariate vector $Z$ is not of the form $Z = X' - X$, hence we need to specify the formula explicitly. 
\begin{Schunk}
\begin{Sinput}
> pim.fit3 <- pim(foodexp ~ I((R(income)-L(income))/sqrt(R(income)+L(income)))-1, 
+   data = Engeldata, link="logit", poset=oldpimposetbft, 
+   estimator=estimator.nleqslv(ignore.error=TRUE), 
+   keep.data=TRUE, interpretation="regular", 
+   extra.nicenames=data.frame(
+     org="I((R(income)-L(income))/sqrt(R(income)+L(income)))", 
+     nice="weightedincomediff", stringsAsFactors=FALSE))
> summary(pim.fit3)				
\end{Sinput}
\begin{Soutput}
Call:
pim(formula = foodexp ~ I((R(income) - L(income))/sqrt(R(income) + 
    L(income))) - 1, data = Engeldata, link = "logit", poset = oldpimposetbft, 
    interpretation = "regular", estimator = estimator.nleqslv(ignore.error = TRUE), 
    keep.data = TRUE, extra.nicenames = data.frame(org = "I((R(income)-L(income))/sqrt(R(income)+L(income)))", 
        nice = "weightedincomediff", stringsAsFactors = FALSE))

                   Estimate Std. Error Z value  Pr(>|z|)    
weightedincomediff  0.38971    0.02436  15.998 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
\end{Soutput}
\end{Schunk}
The estimated model is given by 
\[
\text{logit}\left( \hatprob{FE \leqs FE' } \right) =  0.39 \frac{HI' - HI}{\sqrt{HI' + HI}}.  
\]



\section{Categorical predictors}\label{S_categorical}


In the presence of categorical predictors, a dummy coding is used together with the covariate vector $Z = X'_{dummy} - X_{dummy}$ , where $X_{dummy}$ denotes the dummy coding of the predictor $X$. This model is inspired by the relation between a linear models and a PIM. As an example consider a predictor $X$ with 3 levels $A$, $B$ and $C$ with dummy coding $X_{B} = 1$ if $X=B$ and $X_{B} = 0$ otherwise and $X_C = 1$ if $X = C$ and $X_C = 0$ otherwise. The linear model
\[
Y = \alpha_0 + \alpha_1 X_B + \alpha_2 X_C + \varepsilon,
\] 
with $\varepsilon \sim \text{N}(0,\sigma^2)$ embeds the PIM
\[
\prob{Y \leqs Y'| X, X'} = \Phi\left\{\beta_1 (X'_B - X_B) + \beta_2 (X'_C - X_C) \right\},
\]
where $\beta_i = \alpha_i/\sqrt{2 \sigma^2}$. Note that the PIM has only 2 parameters ($\beta_1$ and $\beta_2$) to model 3 probabilities $\prob{Y \leqs Y'| X = A, X' = B}$, $\prob{Y \leqs Y'| X = A, X' = C}$ and $\prob{Y \leqs Y'| X = B, X' = C}$. This is a consequence of the transitivity assumption which is implied by the linear model:
\begin{eqnarray*}
\prob{Y \leqs Y'| X = B, X' = C} &=& \Phi\left\{ \Phi^{-1} \left(\prob{Y \leqs Y'| X = A, X' = C} \right)  \right. \\
																 & & \left. 	- \Phi^{-1}\left(\prob{Y \leqs Y'| X = A, X' = B} \right) \right\}.
\end{eqnarray*}
In R this becomes
\begin{Schunk}
\begin{Sinput}
> n <- 100
> X <- factor(sample(LETTERS[1:3], n, replace = TRUE))
> Y <- model.matrix(~ X)%*%c(1,2,3) + rnorm(n)
> data.tmp <- data.frame(Y, X)
> pim.fit4 <- pim(Y ~ X-1, data = data.tmp, link = "probit", poset=oldpimposet, 
+   estimator=estimator.nleqslv(ignore.error=TRUE), keep.data=TRUE, 
+   interpretation="regular")
> summary(pim.fit4)
\end{Sinput}
\begin{Soutput}
Call:
pim(formula = Y ~ X - 1, data = data.tmp, link = "probit", poset = oldpimposet, 
    interpretation = "regular", estimator = estimator.nleqslv(ignore.error = TRUE), 
    keep.data = TRUE)

         Estimate Std. Error Z value  Pr(>|z|)    
X_R-_L_B  1.53525    0.21324  7.1996 6.038e-13 ***
X_R-_L_C  2.37825    0.26848  8.8580 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
\end{Soutput}
\end{Schunk}
The estimated model is given by
\[
\Phi^{-1}\left( \hatprob{Y \leqs Y'| X, X'} \right)= 1.54 (X'_B - X_B) + 2.38 (X'_C - X_C).
\]

Note that PIMs are semiparametric, thus the normality assumption is not required to obtain consistent and asymptotically normally distributed estimators. The linear model merely serves as a guide on how $Z$ can be constructed based on the predictors $X$ and $X'$.

\section{Conclusions and remarks}\label{S_conclusion}


The \texttt{pim}-package is illustrated on several examples and allows fitting a broad class of PIMs. PIMs which are embedded by a linear model as well as less restrictive PIMs are allowed. For categorical predictors however, only PIMs which are based on a linear model can be constructed.


Note that for a sample size of $n$ 	a total $n(n-1)/2$ pseudo-observations are created. Consequently for large sample sizes the function goes quite slow.  

In one of the next versions the above mentioned shortcomings will be tackled. All bugs/comments/suggestions are welcome at \href{mailto:JanR.DeNeve@Ugent.be}{JanR.DeNeve@Ugent.be}.


\bibliographystyle{plainnat}
\bibliography{jdeneve}

\end{document}
